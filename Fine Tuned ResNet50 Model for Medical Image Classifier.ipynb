{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal_images\\cyanosis\\Image_1.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the base folder\n",
    "base_folder = 'Multimodal_images'\n",
    "\n",
    "# Use glob to find all jpg files recursively\n",
    "jpg_files = glob.glob(os.path.join(base_folder, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "# Print the list of jpg file paths\n",
    "print(jpg_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(image_paths, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path in image_paths:\n",
    "        image = load_img(path, target_size=target_size)   #module come from tensorflow.keras.preprocessing.image\n",
    "        image = img_to_array(image)  # (height x width x channels) = (224,224,3)\n",
    "        images.append(image)\n",
    "        label = os.path.basename(os.path.dirname(path))  # Assuming folder name is the label\n",
    "        labels.append(label)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multimodal_images\\\\cyanosis\\\\Image_1.jpg'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and their labels\n",
    "image_paths =jpg_files\n",
    "images, labels = load_and_preprocess_images(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cyanosis', 'cyanosis', 'cyanosis', 'cyanosis'], dtype='<U15')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "224\n",
      "224\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(len(images[0]))\n",
    "print(len(images[0][0]))\n",
    "print(len(images[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  9.,  13.,  12.],\n",
       "        [  6.,  10.,   9.],\n",
       "        [  7.,  11.,  10.],\n",
       "        ...,\n",
       "        [177., 226., 205.],\n",
       "        [182., 227., 207.],\n",
       "        [181., 230., 209.]],\n",
       "\n",
       "       [[  6.,  10.,   9.],\n",
       "        [  4.,   8.,   7.],\n",
       "        [  6.,  10.,   9.],\n",
       "        ...,\n",
       "        [177., 226., 205.],\n",
       "        [182., 227., 207.],\n",
       "        [181., 230., 209.]],\n",
       "\n",
       "       [[  7.,  11.,  10.],\n",
       "        [  5.,   9.,   8.],\n",
       "        [  5.,   9.,   8.],\n",
       "        ...,\n",
       "        [179., 228., 207.],\n",
       "        [180., 225., 205.],\n",
       "        [179., 228., 207.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[103., 168., 206.],\n",
       "        [ 92., 156., 192.],\n",
       "        [ 91., 155., 193.],\n",
       "        ...,\n",
       "        [206., 171., 149.],\n",
       "        [201., 174., 147.],\n",
       "        [200., 173., 144.]],\n",
       "\n",
       "       [[ 66., 135., 168.],\n",
       "        [ 55., 121., 155.],\n",
       "        [ 33.,  93., 129.],\n",
       "        ...,\n",
       "        [203., 171., 148.],\n",
       "        [206., 176., 152.],\n",
       "        [201., 175., 150.]],\n",
       "\n",
       "       [[ 59., 125., 159.],\n",
       "        [ 48., 111., 146.],\n",
       "        [ 41.,  99., 136.],\n",
       "        ...,\n",
       "        [203., 171., 148.],\n",
       "        [211., 181., 157.],\n",
       "        [201., 175., 150.]]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 224, 224, 3)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images for ResNet50\n",
    "images = tf.keras.applications.resnet50.preprocess_input(images) \n",
    "#For ResNet50, this involves converting the pixel values from a range of [0, 255] to a range suitable for the model.\n",
    "# \"(1)Convert RGB to BGR and (2)convert center at 0 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -91.939    , -103.779    , -114.68     ],\n",
       "        [ -94.939    , -106.779    , -117.68     ],\n",
       "        [ -93.939    , -105.779    , -116.68     ],\n",
       "        ...,\n",
       "        [ 101.061    ,  109.221    ,   53.32     ],\n",
       "        [ 103.061    ,  110.221    ,   58.32     ],\n",
       "        [ 105.061    ,  113.221    ,   57.32     ]],\n",
       "\n",
       "       [[ -94.939    , -106.779    , -117.68     ],\n",
       "        [ -96.939    , -108.779    , -119.68     ],\n",
       "        [ -94.939    , -106.779    , -117.68     ],\n",
       "        ...,\n",
       "        [ 101.061    ,  109.221    ,   53.32     ],\n",
       "        [ 103.061    ,  110.221    ,   58.32     ],\n",
       "        [ 105.061    ,  113.221    ,   57.32     ]],\n",
       "\n",
       "       [[ -93.939    , -105.779    , -116.68     ],\n",
       "        [ -95.939    , -107.779    , -118.68     ],\n",
       "        [ -95.939    , -107.779    , -118.68     ],\n",
       "        ...,\n",
       "        [ 103.061    ,  111.221    ,   55.32     ],\n",
       "        [ 101.061    ,  108.221    ,   56.32     ],\n",
       "        [ 103.061    ,  111.221    ,   55.32     ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 102.061    ,   51.221    ,  -20.68     ],\n",
       "        [  88.061    ,   39.221    ,  -31.68     ],\n",
       "        [  89.061    ,   38.221    ,  -32.68     ],\n",
       "        ...,\n",
       "        [  45.060997 ,   54.221    ,   82.32     ],\n",
       "        [  43.060997 ,   57.221    ,   77.32     ],\n",
       "        [  40.060997 ,   56.221    ,   76.32     ]],\n",
       "\n",
       "       [[  64.061    ,   18.221    ,  -57.68     ],\n",
       "        [  51.060997 ,    4.2210007,  -68.68     ],\n",
       "        [  25.060997 ,  -23.779    ,  -90.68     ],\n",
       "        ...,\n",
       "        [  44.060997 ,   54.221    ,   79.32     ],\n",
       "        [  48.060997 ,   59.221    ,   82.32     ],\n",
       "        [  46.060997 ,   58.221    ,   77.32     ]],\n",
       "\n",
       "       [[  55.060997 ,    8.221001 ,  -64.68     ],\n",
       "        [  42.060997 ,   -5.7789993,  -75.68     ],\n",
       "        [  32.060997 ,  -17.779    ,  -82.68     ],\n",
       "        ...,\n",
       "        [  44.060997 ,   54.221    ,   79.32     ],\n",
       "        [  53.060997 ,   64.221    ,   87.32     ],\n",
       "        [  46.060997 ,   58.221    ,   77.32     ]]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 224, 224, 3)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder()\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17\n",
      " 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17\n",
      " 17 17 17 17]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "print(label_encoder)\n",
    "#For example, if labels are ['cat', 'dog', 'mouse'], it might convert them to [0, 1, 2]\n",
    "labels_encoded = label_encoder.fit_transform(labels) \n",
    "print(labels_encoded)\n",
    "# len(labels_encoded)\n",
    "#Convert Numerical Labels to One-Hot Encoding:\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "labels_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 18)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_categorical))\n",
    "print(len(labels_categorical[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 model without the top fully connected layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers to avoid retraining them. \n",
    "# Means weight unchange(take weight from orginal renest50) in new sequential model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 39s 3s/step - loss: 21.9223 - accuracy: 0.2871 - val_loss: 11.5860 - val_accuracy: 0.3551\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 33s 2s/step - loss: 4.7754 - accuracy: 0.5647 - val_loss: 4.2136 - val_accuracy: 0.4766\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 31s 2s/step - loss: 1.3118 - accuracy: 0.7976 - val_loss: 2.9762 - val_accuracy: 0.5421\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 32s 2s/step - loss: 0.7281 - accuracy: 0.8188 - val_loss: 2.7755 - val_accuracy: 0.5047\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 30s 2s/step - loss: 0.3141 - accuracy: 0.9129 - val_loss: 1.9217 - val_accuracy: 0.6449\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 30s 2s/step - loss: 0.0733 - accuracy: 0.9718 - val_loss: 2.4681 - val_accuracy: 0.5701\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 30s 2s/step - loss: 0.0305 - accuracy: 0.9929 - val_loss: 2.3348 - val_accuracy: 0.6262\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 30s 2s/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 1.9721 - val_accuracy: 0.6729\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 30s 2s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.7685 - val_accuracy: 0.6822\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 32s 2s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.7398 - val_accuracy: 0.6729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f530c26b00>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 1.7398 - accuracy: 0.6729\n",
      "Test accuracy: 67.29%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#model.save('custom_resnet50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 906ms/step\n",
      "hand lump\n"
     ]
    }
   ],
   "source": [
    "image, labels = load_and_preprocess_images(['images1.jpg'])\n",
    "    \n",
    "# Predict probabilities\n",
    "preds = model.predict(image)\n",
    "    \n",
    "# Get the predicted class label\n",
    "pred_class = np.argmax(preds)\n",
    "    \n",
    "# Map predicted class index to class label\n",
    "pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
